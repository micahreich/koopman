{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3])\n",
      "torch.Size([10, 3])\n",
      "(tensor([[ 1.4862, -1.0360, -0.5143],\n",
      "        [-1.1530,  1.2557,  0.2224],\n",
      "        [-0.6994,  0.8583,  1.3914],\n",
      "        [-0.1733, -0.0907,  1.1319],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [-1.8744,  4.4257,  2.1459],\n",
      "        [-1.3158,  0.1616, -0.0959],\n",
      "        [-0.0464,  0.0097,  0.1212],\n",
      "        [-2.0655,  0.1496,  1.5749],\n",
      "        [ 0.0000,  0.0000,  0.0000]], grad_fn=<MulBackward0>),)\n",
      "tensor([[ 1.4862, -1.0360, -0.5143]], grad_fn=<MmBackward0>)\n",
      "tensor([[-1.1530,  1.2557,  0.2224]], grad_fn=<MmBackward0>)\n",
      "tensor([[-0.6994,  0.8583,  1.3914]], grad_fn=<MmBackward0>)\n",
      "tensor([[-0.1733, -0.0907,  1.1319]], grad_fn=<MmBackward0>)\n",
      "tensor([[-1.8744,  4.4257,  2.1459]], grad_fn=<MmBackward0>)\n",
      "tensor([[-1.3158,  0.1616, -0.0959]], grad_fn=<MmBackward0>)\n",
      "tensor([[-0.0464,  0.0097,  0.1212]], grad_fn=<MmBackward0>)\n",
      "tensor([[-2.0655,  0.1496,  1.5749]], grad_fn=<MmBackward0>)\n",
      "tensor([[ 1.4862, -1.0360, -0.5143]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from torch.autograd.functional import jvp\n",
    "\n",
    "batch_size = 10\n",
    "nx = 3\n",
    "\n",
    "A = torch.randn(nx, nx, requires_grad=True)\n",
    "B = torch.randn(nx, nx, requires_grad=True)\n",
    "\n",
    "xs = torch.randn(batch_size, nx, requires_grad=True)\n",
    "zs = torch.exp(xs)\n",
    "\n",
    "inds = [0, 1, 2, 3, 5, 6, 7, 8]\n",
    "zs_next = zs[inds, :] @ B.T\n",
    "\n",
    "print(zs_next.shape)\n",
    "print(xs.shape)\n",
    "\n",
    "vsx = torch.randn(zs_next.shape[0], 3)\n",
    "jx = torch.autograd.grad(zs_next, xs, grad_outputs=vsx, create_graph=True)\n",
    "print(jx)\n",
    "for j, i in enumerate(inds):\n",
    "    print(\n",
    "        vsx[None, j, ...] @ B @ torch.diag(\n",
    "            torch.exp(xs[i, :])\n",
    "        )\n",
    "    )\n",
    "    \n",
    "zz = zs_next[0, :]\n",
    "xx = xs[0, :]\n",
    "\n",
    "jac = B @ torch.diag(torch.exp(xs[0]))\n",
    "print(vsx[0][None, :] @ jac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 3]) torch.Size([7, 2]) torch.Size([7, 3])\n",
      "tensor(7.8979, grad_fn=<MeanBackward1>) tensor(7.3632, grad_fn=<PowBackward0>)\n",
      "tensor(9.3245, grad_fn=<MeanBackward1>) tensor(8.6587, grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from torch.autograd.functional import jvp\n",
    "\n",
    "nx = 3\n",
    "nu = 2\n",
    "batch = 7\n",
    "\n",
    "A = torch.randn(nx, nx, requires_grad=True)\n",
    "B = torch.randn(nx, nu, requires_grad=True)\n",
    "\n",
    "\n",
    "def f(x, u):\n",
    "    return x @ A.T + u @ B.T\n",
    "\n",
    "x = torch.randn(batch, nx, requires_grad=True)\n",
    "u = torch.randn(batch, nu, requires_grad=True)\n",
    "xu = torch.cat([x, u], dim=-1)\n",
    "\n",
    "y = f(x, u)\n",
    "print(x.shape, u.shape, y.shape)\n",
    "\n",
    "n_proj = 10\n",
    "vsx = torch.randn(batch, n_proj, nx, requires_grad=True)\n",
    "vsu = torch.randn(batch, n_proj, nx, requires_grad=True)\n",
    "\n",
    "grads_x = torch.zeros(batch,)\n",
    "grads_u = torch.zeros(batch,)\n",
    "\n",
    "for i in range(n_proj):\n",
    "    Gx, Gu = torch.autograd.grad(y, (x, u), grad_outputs=vsx[:, i, :], create_graph=True)\n",
    "        \n",
    "    grads_x += 1/n_proj * (torch.norm(Gx, 2, dim=-1) ** 2)\n",
    "    grads_u += 1/n_proj * (torch.norm(Gu, 2, dim=-1) ** 2)\n",
    "\n",
    "grads_x_mean = grads_x.mean(dim=0)\n",
    "grads_u_mean = grads_u.mean(dim=0)\n",
    "\n",
    "print(grads_x_mean, torch.norm(A, 'fro') ** 2)\n",
    "print(grads_u_mean, torch.norm(B, 'fro') ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from torch.autograd.functional import jvp\n",
    "\n",
    "# Example network: mapping from R^10 to R^20.\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(10, 32),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(32, 20)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = SimpleModel()\n",
    "\n",
    "# Dummy input: a batch of 3 samples.\n",
    "x = torch.randn(3, 10, requires_grad=True)\n",
    "\n",
    "# Forward pass: compute output f(x) (shape: [3, 20])\n",
    "output = model(x)\n",
    "\n",
    "# Sample a random vector v with the same shape as output.\n",
    "# For a proper unbiased estimator, v could be chosen with entries Â±1 or Gaussian.\n",
    "v = torch.randn_like(output)\n",
    "\n",
    "# Compute the dot product between output and v, yielding a scalar.\n",
    "# This is equivalent to summing over all samples and features.\n",
    "scalar = torch.sum(output * v)\n",
    "\n",
    "# Compute the gradient of this scalar with respect to x.\n",
    "# This gradient is the Jacobian-vector product J_f(x)^T * v.\n",
    "jvp_val = torch.autograd.grad(scalar, x, create_graph=True)[0]\n",
    "\n",
    "# Now, you can compute a norm on the jvp (for example, its squared L2 norm)\n",
    "jvp_norm_squared = torch.sum(jvp_val ** 2)\n",
    "\n",
    "# Use this as a regularization loss term.\n",
    "loss_reg = jvp_norm_squared\n",
    "\n",
    "# Total loss combines your task loss and the Jacobian regularization\n",
    "total_loss = loss_reg\n",
    "\n",
    "# Backward pass:\n",
    "total_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time per trial: 0.000413 seconds\n"
     ]
    }
   ],
   "source": [
    "n_trials = 1000\n",
    "start = time.perf_counter()\n",
    "\n",
    "xs = [\n",
    "    torch.randn(3, 10, requires_grad=True) for _ in range(n_trials)\n",
    "]\n",
    "\n",
    "for i in range(n_trials):\n",
    "    # Dummy input: a batch of 3 samples.\n",
    "    x = xs[i]\n",
    "    output = model(x)\n",
    "\n",
    "    v = torch.randn_like(output)\n",
    "    scalar = torch.sum(output * v)\n",
    "\n",
    "    # Compute the gradient of this scalar with respect to x.\n",
    "    # This gradient is the Jacobian-vector product J_f(x)^T * v.\n",
    "    jvp_val = torch.autograd.grad(scalar, x, create_graph=True)[0]\n",
    "    jvp_norm_squared = torch.sum(jvp_val ** 2)\n",
    "    total_loss = jvp_norm_squared\n",
    "\n",
    "    # Backward pass:\n",
    "    total_loss.backward()\n",
    "\n",
    "end = time.perf_counter()\n",
    "print(f\"Time per trial: {(end - start) / n_trials:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time per trial: 0.000764 seconds\n"
     ]
    }
   ],
   "source": [
    "n_trials = 1000\n",
    "start = time.perf_counter()\n",
    "\n",
    "def f(x):\n",
    "    return model(x)\n",
    "\n",
    "for _ in range(n_trials):\n",
    "    # Dummy input: a batch of 3 samples.\n",
    "    x = xs[i]\n",
    "    v = torch.randn_like(x)\n",
    "    output, jvp_val = jvp(f, (x,), (v,), create_graph=True)\n",
    "    jvp_norm_squared = torch.sum(jvp_val ** 2)\n",
    "    total_loss = jvp_norm_squared\n",
    "\n",
    "    # Backward pass:\n",
    "    total_loss.backward()\n",
    "\n",
    "end = time.perf_counter()\n",
    "print(f\"Time per trial: {(end - start) / n_trials:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6]) torch.Size([1, 100, 6])\n",
      "tensor(0.1704, grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import einops\n",
    "\n",
    "batch_size = 1\n",
    "nx = 6\n",
    "ny = nx\n",
    "nproj = 100\n",
    "\n",
    "def f(x):\n",
    "    return 2*x\n",
    "\n",
    "x = torch.randn(batch_size, nx, requires_grad=True)\n",
    "y = f(x)\n",
    "\n",
    "vs = torch.randn(batch_size, nproj, ny, requires_grad=True)\n",
    "\n",
    "print(y.shape, vs.shape)\n",
    "\n",
    "dot_products = torch.einsum('bpm,bm->bp', vs, y)  # shape: (B, P)\n",
    "mean_dot = dot_products.mean(dim=-1)\n",
    "\n",
    "grads = torch.autograd.grad(mean_dot, x, grad_outputs=torch.ones_like(mean_dot), create_graph=True)[0]  # shape: (B, nx)\n",
    "grads_squared_norm = torch.norm(grads, dim=-1) ** 2\n",
    "mean_grads_squared_norm = grads_squared_norm.mean(dim=0)  # shape: (nx,)\n",
    "\n",
    "print(mean_grads_squared_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "# analytic_1 = 1/3 * (\n",
    "#     vs[0, 0, :] @ (2*torch.eye(ny)) #+ \\\n",
    "#     # vs[0, 1, :] @ (2*torch.eye(ny)) + \\\n",
    "#     # vs[0, 2, :] @ (2*torch.eye(ny))\n",
    "# )\n",
    "\n",
    "# print(analytic_1)\n",
    "print(grads.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(68.8100, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "nx = 6\n",
    "ny = nx\n",
    "nproj = 100\n",
    "\n",
    "def f(x):\n",
    "    return 2*x\n",
    "\n",
    "seed = 42\n",
    "torch.random.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "vs = torch.randn(batch_size, nproj, ny, requires_grad=True)\n",
    "\n",
    "class JacobianReg(nn.Module):\n",
    "    '''\n",
    "    Loss criterion that computes the trace of the square of the Jacobian.\n",
    "\n",
    "    Arguments:\n",
    "        n (int, optional): determines the number of random projections.\n",
    "            If n=-1, then it is set to the dimension of the output \n",
    "            space and projection is non-random and orthonormal, yielding \n",
    "            the exact result.  For any reasonable batch size, the default \n",
    "            (n=1) should be sufficient.\n",
    "    '''\n",
    "    def __init__(self, n=1):\n",
    "        assert n == -1 or n > 0\n",
    "        self.n = n\n",
    "        super(JacobianReg, self).__init__()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        '''\n",
    "        computes (1/2) tr |dy/dx|^2\n",
    "        '''\n",
    "        B, C = y.shape\n",
    "        if self.n == -1:\n",
    "            num_proj = C\n",
    "        else:\n",
    "            num_proj = self.n\n",
    "        J2 = 0\n",
    "        for ii in range(num_proj):\n",
    "            if self.n == -1:\n",
    "                # orthonormal vector, sequentially spanned\n",
    "                v = torch.zeros(B, C)\n",
    "                v[:, ii] = 1\n",
    "            else:\n",
    "                # random properly-normalized vector for each sample\n",
    "                # v = self._random_vector(C=C, B=B)\n",
    "                v = vs[:, ii, :]\n",
    "            if x.is_cuda:\n",
    "                v = v.cuda()\n",
    "            Jv = self._jacobian_vector_product(y, x, v, create_graph=True)\n",
    "            J2 += C * torch.norm(Jv) ** 2 / (num_proj * B)\n",
    "        R = (1 / 2) * J2\n",
    "        return R\n",
    "\n",
    "    def _random_vector(self, C, B):\n",
    "        '''\n",
    "        creates a random vector of dimension C with a norm of C^(1/2)\n",
    "        (as needed for the projection formula to work)\n",
    "        '''\n",
    "        # if C == 1:\n",
    "        #     return torch.ones(B)\n",
    "        # v = torch.randn(B, C)\n",
    "        # arxilirary_zero = torch.zeros(B, C)\n",
    "        # vnorm = torch.norm(v, 2, 1, True)\n",
    "        # v = torch.addcdiv(arxilirary_zero, 1.0, v, vnorm)\n",
    "        # return v\n",
    "        # return vs\n",
    "        pass\n",
    "    \n",
    "    def _jacobian_vector_product(self, y, x, v, create_graph=False):\n",
    "        '''\n",
    "        Produce jacobian-vector product dy/dx dot v.\n",
    "\n",
    "        Note that if you want to differentiate it,\n",
    "        you need to make create_graph=True\n",
    "        '''\n",
    "        flat_y = y.reshape(-1)\n",
    "        flat_v = v.reshape(-1)\n",
    "        grad_x, = torch.autograd.grad(flat_y, x, flat_v, retain_graph=True, create_graph=create_graph)\n",
    "        return grad_x\n",
    "\n",
    "\n",
    "jacreg = JacobianReg(n=nproj)\n",
    "x = torch.randn(batch_size, nx, requires_grad=True)\n",
    "y = f(x)\n",
    "\n",
    "nrm = jacreg.forward(x, y)\n",
    "print(nrm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3]) torch.Size([2, 3])\n",
      "tensor(0.0589, grad_fn=<MeanBackward1>)\n",
      "tensor(5.)\n"
     ]
    }
   ],
   "source": [
    "nx = 3\n",
    "batch = 2\n",
    "\n",
    "A = torch.diag(torch.arange(nx * 1.0))\n",
    "\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "def f(x):\n",
    "    return x @ A.T\n",
    "\n",
    "x = torch.randn(batch, nx, requires_grad=True)\n",
    "y = f(x)\n",
    "\n",
    "# vs = torch.randn(batch, nx, requires_grad=True)\n",
    "\n",
    "# G = torch.autograd.grad(y, x, grad_outputs=vs, create_graph=True)[0]\n",
    "# print(G.shape)\n",
    "# print(G)\n",
    "\n",
    "# for i in range(batch):\n",
    "#     vT = vs[i, :].reshape((1, -1))\n",
    "#     print(vT @ A)\n",
    "    \n",
    "n_proj = 10\n",
    "vs = torch.randn(batch, n_proj, nx, requires_grad=True)\n",
    "# vs = vs / torch.norm(vs, dim=-1, keepdim=True)\n",
    "\n",
    "grads = torch.zeros(batch,)\n",
    "\n",
    "for i in range(n_proj):\n",
    "    G = torch.autograd.grad(y, x, grad_outputs=vs[:, i, :], create_graph=True)[0]    \n",
    "    grads += 1/nproj * (torch.norm(G, 2, dim=-1) ** 2)\n",
    "\n",
    "grads_mean = grads.mean(dim=0)\n",
    "print(grads_mean)\n",
    "print(torch.norm(A, 'fro') ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(17.8793, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch.func import jvp\n",
    "\n",
    "jvp_gradx = torch.zeros(batch_size)\n",
    "# vs = torch.randn(batch_size, nproj, nx)\n",
    "\n",
    "for i in range(nproj):\n",
    "    value, grad = jvp(f, (x,), (vs[:, i, :],))\n",
    "    jvp_gradx += nx * torch.norm(grad)**2 / nproj\n",
    "\n",
    "jvp_gradx = jvp_gradx.mean(dim=0)\n",
    "print(jvp_gradx / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2607, 0.4251])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor(47.6827)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "nx = 6\n",
    "nu = 3\n",
    "nproj = 1000\n",
    "\n",
    "A = torch.randn(nx, nx)\n",
    "B = torch.randn(nx, nu)\n",
    "\n",
    "def f(x, u):\n",
    "    return x @ A.T + u @ B.T\n",
    "\n",
    "jvp_gradx = torch.zeros(batch_size)\n",
    "jvp_gradu = torch.zeros(batch_size)\n",
    "\n",
    "vsx = torch.randn(batch_size, nproj, nx)\n",
    "vsu = torch.randn(batch_size, nproj, nu)\n",
    "\n",
    "xs = torch.randn(batch_size, nx, requires_grad=True)\n",
    "us = torch.randn(batch_size, nu, requires_grad=True)\n",
    "zs = xs @ A.T + us @ B.T\n",
    "\n",
    "for i in range(nproj):\n",
    "    value, grad = jvp(lambda x, u: zs, (xs, us), (vsx[:, i, :], vsu[:, i, :]))\n",
    "    jvp_gradx += torch.norm(grad)**2 / nproj\n",
    "\n",
    "jvp_gradx = jvp_gradx.mean(dim=0)\n",
    "mat_AB = torch.cat([A, B], dim=-1)\n",
    "\n",
    "print(jvp_gradx)\n",
    "print(torch.norm(mat_AB, 'fro')**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
