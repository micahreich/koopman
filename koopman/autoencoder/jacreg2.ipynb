{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import einops\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from koopman import utils\n",
    "from koopman.autoencoder.dataset import KoopmanDataset\n",
    "from koopman.autoencoder.model import KoopmanAutoencoder\n",
    "from koopman.simulation.systems import NonlinearAttractor2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9387, grad_fn=<MeanBackward1>) tensor(12.3386, grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "pred_horizon = 20\n",
    "batch_size = 32\n",
    "\n",
    "data = torch.load('/home/mreich/workspaces/koopman/koopman/autoencoder/attractor/attractor_data.pt')\n",
    "ts = data[\"ts\"]\n",
    "xhist = data[\"xhist\"]\n",
    "uhist = data[\"uhist\"]\n",
    "dt = data[\"dt\"]\n",
    "\n",
    "dataset = KoopmanDataset(xhist, uhist, ts, pred_horizon, dt)\n",
    "\n",
    "model = KoopmanAutoencoder(\n",
    "    nx=NonlinearAttractor2D.nx,\n",
    "    nu=NonlinearAttractor2D.nu,\n",
    "    nz=3,\n",
    "    H=pred_horizon,\n",
    "    params_init='eye',\n",
    "    hidden_dims=[32, 32],\n",
    "    activation=nn.Mish,\n",
    "    use_layernorm=False,\n",
    "    horizon_loss_weight=5.0,\n",
    "    L1_loss_weight=0.5,\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "xhist, uhist = next(iter(dataloader))\n",
    "\n",
    "xhist = xhist.clone().detach().requires_grad_(True)\n",
    "uhist = uhist.clone().detach().requires_grad_(True)\n",
    "\n",
    "xhist_flat = einops.rearrange(xhist, 'b h nx -> (b h) nx')\n",
    "uhist_flat = einops.rearrange(uhist, 'b h nu -> (b h) nu')\n",
    "zhist_flat = model.forward(xhist_flat)\n",
    "\n",
    "jx, ju = model._dynamics_jacobian_norm(xhist_flat, uhist_flat, zhist_flat)\n",
    "\n",
    "print(jx, ju)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "jx.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 62\u001b[0m\n\u001b[1;32m     51\u001b[0m xs_of_interest_flat \u001b[38;5;241m=\u001b[39m einops\u001b[38;5;241m.\u001b[39mrearrange(xs_of_interest, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb h nx -> (b h) nx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# zs_next_flat = 5.0 * zs_flat + us_flat @ B.T\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# xs_next_flat = zs_next_flat[:, :10]\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# vsx = torch.randn(bh_size, 10)\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m jx \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs_next_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxs_of_interest_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvsx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspaces/koopman/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:496\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    492\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    493\u001b[0m         grad_outputs_\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    507\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    508\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    509\u001b[0m     ):\n",
      "File \u001b[0;32m~/workspaces/koopman/venv/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior."
     ]
    }
   ],
   "source": [
    "# Example network: mapping from R^10 to R^20.\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(10, 32),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(32, 20)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = SimpleModel()\n",
    "\n",
    "batch_size = 16\n",
    "h = 10\n",
    "nx = 10\n",
    "nu = 4\n",
    "\n",
    "# fwd pass\n",
    "xs = torch.randn(batch_size, h+1, nx)\n",
    "# xs.requires_grad_(True)\n",
    "\n",
    "us = torch.randn(batch_size, h, nu)\n",
    "# us.requires_grad_(True)\n",
    "\n",
    "xs_flat = einops.rearrange(xs, 'b h nx -> (b h) nx')\n",
    "us_flat = einops.rearrange(us, 'b h nu -> (b h) nu')\n",
    "zs_flat = model(xs_flat)\n",
    "\n",
    "B = torch.randn(20, nu)\n",
    "\n",
    "zs = einops.rearrange(zs_flat, '(b h) nz -> b h nz', b=batch_size)\n",
    "\n",
    "zs_of_interest = zs[:, :-1, :]\n",
    "zs_of_interest_flat = einops.rearrange(zs_of_interest, 'b h nz -> (b h) nz')\n",
    "\n",
    "zs_next_flat = 5.0 * zs_of_interest_flat + us_flat @ B.T\n",
    "xs_next_flat = zs_next_flat[:, :10]\n",
    "\n",
    "bh_size = zs_next_flat.shape[0]\n",
    "vsx = torch.randn(bh_size, 10)\n",
    "\n",
    "xs_of_interest = xs[:, :-1, :].detach().requires_grad_(True)\n",
    "xs_of_interest_flat = einops.rearrange(xs_of_interest, 'b h nx -> (b h) nx')\n",
    "\n",
    "jx = torch.autograd.grad(xs_next_flat, xs_of_interest_flat, grad_outputs=vsx, create_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
