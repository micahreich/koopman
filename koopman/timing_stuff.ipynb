{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd.functional import jvp, jacobian\n",
    "import numpy as np\n",
    "from time import perf_counter\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0.014265583005908411\n",
      "Time:  0.020336519999546\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ntrials = 1000\n",
    "A = torch.randn(50, 50, device=device)\n",
    "X = torch.randn(1_000, 50, device=device)\n",
    "\n",
    "_ = X @ A.T\n",
    "_ = einops.einsum(A, X, 'i j, k j -> k i')\n",
    "\n",
    "start = perf_counter()\n",
    "for _ in range(ntrials):\n",
    "    y = X @ A.T\n",
    "end = perf_counter()\n",
    "print(\"Time: \", end - start)\n",
    "\n",
    "start = perf_counter()\n",
    "for _ in range(ntrials):\n",
    "    y = einops.einsum(A, X, 'i j, k j -> k i')\n",
    "end = perf_counter()\n",
    "print(\"Time: \", end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 1:  0.005075670477002859\n",
      "Time 2:  0.004025802748001297\n"
     ]
    }
   ],
   "source": [
    "# A simple feedforward model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Function to compute Jv (Jacobian-vector product)\n",
    "def compute_jacobian_penalty1(model, x, n_projections=1):\n",
    "    def f(x_):\n",
    "        return model(x_)\n",
    "\n",
    "    penalty = 0.0    \n",
    "    for _ in range(n_projections):\n",
    "        v = torch.randn_like(x)  # random direction\n",
    "        v = v / (v.norm(dim=1, keepdim=True) + 1e-8)  # normalize for stability\n",
    "\n",
    "        _, jvp_result = jvp(f, (x,), (v,), create_graph=True)\n",
    "\n",
    "        # L2 norm squared of the directional derivative\n",
    "        penalty += (jvp_result ** 2).sum(dim=1).mean()\n",
    "\n",
    "    return penalty / n_projections\n",
    "\n",
    "def compute_jacobian_penalty2(model, x, n_projections=1):\n",
    "    def f(x_):\n",
    "        return model(x_)\n",
    "        \n",
    "    penalty = 0.0\n",
    "    J = jacobian(f, x, create_graph=True)\n",
    "\n",
    "    for _ in range(n_projections):\n",
    "        v = torch.randn_like(x)  # random direction\n",
    "        v = v / (v.norm(dim=1, keepdim=True) + 1e-8)  # normalize for stability\n",
    "\n",
    "        jvp_result = J @ v.T\n",
    "        # _, jvp_result = jvp(f, (x,), (v,), create_graph=True)\n",
    "\n",
    "        # L2 norm squared of the directional derivative\n",
    "        penalty += (jvp_result ** 2).sum(dim=1).mean()\n",
    "\n",
    "    return penalty / n_projections\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 10\n",
    "hidden_dim = 64\n",
    "output_dim = 30\n",
    "model = Net(input_dim, hidden_dim, output_dim)\n",
    "_ = compute_jacobian_penalty1(model, torch.randn(1, input_dim))\n",
    "\n",
    "start = perf_counter()\n",
    "for _ in range(1000):\n",
    "    x = torch.randn(1, input_dim)\n",
    "    l = compute_jacobian_penalty1(model, x, n_projections=20)\n",
    "    l.backward()\n",
    "end = perf_counter()\n",
    "print(\"Time 1: \", (end - start)/1000)\n",
    "\n",
    "_ = compute_jacobian_penalty2(model, torch.randn(1, input_dim))\n",
    "\n",
    "start = perf_counter()\n",
    "for _ in range(1000):\n",
    "    x = torch.randn(1, input_dim)\n",
    "    l = compute_jacobian_penalty2(model, x, n_projections=20)\n",
    "    l.backward()\n",
    "end = perf_counter()\n",
    "print(\"Time 2: \", (end - start)/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return torch.sin(x)\n",
    "\n",
    "n = 20\n",
    "x = torch.randn((32, n), requires_grad=True)\n",
    "jacobians = torch.stack([jacobian(f, x[i]) for i in range(x.shape[0])])\n",
    "print(jacobians.shape)\n",
    "\n",
    "assert torch.allclose(\n",
    "    jacobians[0],\n",
    "    jacobian(f, x[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.82  -0.645  2.915  2.522  1.944 -6.129  1.915 -0.308  0.352  2.207]\n",
      " [-0.645  3.752 -2.887 -0.948  1.736 -0.905  0.407 -1.12  -0.711 -0.411]\n",
      " [ 2.915 -2.887 17.034 -6.815  0.54  -7.634  3.163  5.359  2.248  4.239]\n",
      " [ 2.522 -0.948 -6.815 15.145  0.577  5.117 -5.266 -4.68   1.255  0.245]\n",
      " [ 1.944  1.736  0.54   0.577  4.355 -0.798 -0.741 -2.336  0.869  1.189]\n",
      " [-6.129 -0.905 -7.634  5.117 -0.798 16.148 -4.797  0.346  2.728  0.125]\n",
      " [ 1.915  0.407  3.163 -5.266 -0.741 -4.797 13.306  6.837 -4.172  4.336]\n",
      " [-0.308 -1.12   5.359 -4.68  -2.336  0.346  6.837 10.972  0.955  4.711]\n",
      " [ 0.352 -0.711  2.248  1.255  0.869  2.728 -4.172  0.955  3.879  1.377]\n",
      " [ 2.207 -0.411  4.239  0.245  1.189  0.125  4.336  4.711  1.377  5.42 ]]\n",
      "tensor([[ 7.7136, -0.6979,  2.8631,  2.5697,  1.8827, -5.8404,  1.8831, -0.1574,\n",
      "          0.4195,  2.2244],\n",
      "        [-0.6979,  3.6852, -2.9118, -0.8948,  1.6696, -0.8958,  0.3533, -1.2211,\n",
      "         -0.7452, -0.4812],\n",
      "        [ 2.8631, -2.9118, 17.0376, -6.8457,  0.5491, -7.4930,  3.1198,  5.4068,\n",
      "          2.3063,  4.2593],\n",
      "        [ 2.5697, -0.8948, -6.8457, 15.1732,  0.5565,  5.0783, -5.1878, -4.6251,\n",
      "          1.2406,  0.2527],\n",
      "        [ 1.8827,  1.6696,  0.5491,  0.5565,  4.3865, -0.7156, -0.7492, -2.3969,\n",
      "          0.8442,  1.1589],\n",
      "        [-5.8404, -0.8958, -7.4930,  5.0783, -0.7156, 15.8479, -4.5774,  0.4467,\n",
      "          2.6880,  0.2652],\n",
      "        [ 1.8831,  0.3533,  3.1198, -5.1878, -0.7492, -4.5774, 13.2664,  6.9238,\n",
      "         -4.0774,  4.3992],\n",
      "        [-0.1574, -1.2211,  5.4068, -4.6251, -2.3969,  0.4467,  6.9238, 11.2325,\n",
      "          1.0599,  4.8799],\n",
      "        [ 0.4195, -0.7452,  2.3063,  1.2406,  0.8442,  2.6880, -4.0774,  1.0599,\n",
      "          3.8718,  1.4360],\n",
      "        [ 2.2244, -0.4812,  4.2593,  0.2527,  1.1589,  0.2652,  4.3992,  4.8799,\n",
      "          1.4360,  5.4988]])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "cov = torch.randn(10, 10)\n",
    "cov = cov @ cov.T\n",
    "\n",
    "X = np.random.multivariate_normal(np.zeros(10), cov, 10_000)\n",
    "\n",
    "cov_recon = np.cov(X, rowvar=False)\n",
    "\n",
    "print(cov_recon)\n",
    "print(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5513,  1.1239, -0.3528, -0.6187,  0.1626, -0.9629,  1.3004,  0.0809,\n",
      "        -0.7823, -0.2454], grad_fn=<TanhBackwardBackward0>)\n"
     ]
    }
   ],
   "source": [
    "xs = torch.randn(10, requires_grad=True)\n",
    "ys = torch.tanh(xs)\n",
    "\n",
    "v = torch.randn(10)\n",
    "_, out = jvp(lambda xs: torch.tanh(xs), (xs,), (v,), create_graph=True)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
